"""
å¤šæ ¼å¼æœ¬åœ°çŸ¥è¯†åº“ç³»ç»Ÿ
æ”¯æŒPDFã€Wordã€æ–‡æœ¬æ–‡ä»¶ç­‰æ ¼å¼
é›†æˆå‘é‡æœç´¢å’Œè¯­ä¹‰æ£€ç´¢
"""

import os
import json
import pickle
import hashlib
from typing import List, Dict, Tuple, Optional
from datetime import datetime
import logging

# æ–‡æ¡£å¤„ç†ç›¸å…³
import PyPDF2
import docx
from docx import Document
import pandas as pd

# å‘é‡å¤„ç†ç›¸å…³
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    VECTOR_AVAILABLE = True
except ImportError:
    VECTOR_AVAILABLE = False
    print("âš ï¸ å‘é‡æœç´¢åº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å…³é”®è¯æœç´¢")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ - æ”¯æŒå¤šç§æ ¼å¼"""
    
    @staticmethod
    def extract_text_from_pdf(file_path: str) -> str:
        """ä»PDFæ–‡ä»¶æå–æ–‡æœ¬"""
        try:
            text = ""
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
            return text.strip()
        except Exception as e:
            logger.error(f"PDFå¤„ç†å¤±è´¥ {file_path}: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_docx(file_path: str) -> str:
        """ä»Wordæ–‡æ¡£æå–æ–‡æœ¬"""
        try:
            doc = Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text.strip()
        except Exception as e:
            logger.error(f"Wordæ–‡æ¡£å¤„ç†å¤±è´¥ {file_path}: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_txt(file_path: str, encoding: str = 'utf-8') -> str:
        """ä»æ–‡æœ¬æ–‡ä»¶æå–æ–‡æœ¬"""
        try:
            with open(file_path, 'r', encoding=encoding) as file:
                return file.read().strip()
        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–ç¼–ç 
            encodings = ['gbk', 'gb2312', 'latin-1']
            for enc in encodings:
                try:
                    with open(file_path, 'r', encoding=enc) as file:
                        return file.read().strip()
                except:
                    continue
            logger.error(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: ç¼–ç é—®é¢˜")
            return ""
        except Exception as e:
            logger.error(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_csv(file_path: str) -> str:
        """ä»CSVæ–‡ä»¶æå–æ–‡æœ¬"""
        try:
            df = pd.read_csv(file_path)
            # å°†DataFrameè½¬æ¢ä¸ºæ–‡æœ¬æè¿°
            text = f"CSVæ–‡ä»¶å†…å®¹æ‘˜è¦ï¼š\n"
            text += f"å…±{len(df)}è¡Œï¼Œ{len(df.columns)}åˆ—\n"
            text += f"åˆ—åï¼š{', '.join(df.columns)}\n\n"
            
            # æ·»åŠ å‰å‡ è¡Œæ•°æ®ä½œä¸ºç¤ºä¾‹
            text += "æ•°æ®ç¤ºä¾‹ï¼š\n"
            text += df.head(5).to_string(index=False)
            
            return text
        except Exception as e:
            logger.error(f"CSVæ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {e}")
            return ""
    
    @classmethod
    def extract_text(cls, file_path: str) -> Tuple[str, str]:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åæå–æ–‡æœ¬"""
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext == '.pdf':
            return cls.extract_text_from_pdf(file_path), 'pdf'
        elif ext in ['.docx', '.doc']:
            return cls.extract_text_from_docx(file_path), 'docx'
        elif ext in ['.txt', '.md', '.py', '.js', '.html', '.css', '.json']:
            return cls.extract_text_from_txt(file_path), 'text'
        elif ext == '.csv':
            return cls.extract_text_from_csv(file_path), 'csv'
        else:
            logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")
            return "", 'unknown'

class TextSplitter:
    """æ–‡æœ¬åˆ†å‰²å™¨"""
    
    @staticmethod
    def split_by_paragraphs(text: str, max_length: int = 500, overlap: int = 50) -> List[str]:
        """æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬ï¼Œæ”¯æŒé‡å """
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            # å¦‚æœå•ä¸ªæ®µè½å¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
            if len(paragraph) > max_length:
                sub_chunks = TextSplitter.split_by_sentences(paragraph, max_length)
                for sub_chunk in sub_chunks:
                    if len(current_chunk) + len(sub_chunk) <= max_length:
                        current_chunk += sub_chunk + "\n"
                    else:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = sub_chunk + "\n"
            else:
                if len(current_chunk) + len(paragraph) <= max_length:
                    current_chunk += paragraph + "\n\n"
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = paragraph + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # æ·»åŠ é‡å 
        if overlap > 0 and len(chunks) > 1:
            overlapped_chunks = []
            for i, chunk in enumerate(chunks):
                if i == 0:
                    overlapped_chunks.append(chunk)
                else:
                    # ä»å‰ä¸€ä¸ªchunkçš„æœ«å°¾å–ä¸€äº›æ–‡æœ¬ä½œä¸ºé‡å 
                    prev_chunk = chunks[i-1]
                    overlap_text = prev_chunk[-overlap:] if len(prev_chunk) > overlap else prev_chunk
                    overlapped_chunks.append(overlap_text + "\n" + chunk)
            return overlapped_chunks
        
        return chunks
    
    @staticmethod
    def split_by_sentences(text: str, max_length: int = 500) -> List[str]:
        """æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬"""
        import re
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk) + len(sentence) <= max_length:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

class VectorSearchEngine:
    """å‘é‡æœç´¢å¼•æ“"""
    
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        if not VECTOR_AVAILABLE:
            raise ImportError("å‘é‡æœç´¢ä¾èµ–æœªå®‰è£…")
        
        self.model = SentenceTransformer(model_name)
        self.embeddings = None
        self.documents = []
    
    def build_index(self, documents: List[Dict]):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        self.documents = documents
        texts = [doc['content'] for doc in documents]
        
        logger.info(f"æ­£åœ¨æ„å»º {len(texts)} ä¸ªæ–‡æ¡£çš„å‘é‡ç´¢å¼•...")
        self.embeddings = self.model.encode(texts, show_progress_bar=True)
        logger.info("å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ")
    
    def search(self, query: str, top_k: int = 5, threshold: float = 0.3) -> List[Dict]:
        """å‘é‡æœç´¢"""
        if self.embeddings is None:
            return []
        
        query_embedding = self.model.encode([query])
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        # è·å–æœ€ç›¸å…³çš„æ–‡æ¡£
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > threshold:
                result = self.documents[idx].copy()
                result['score'] = float(similarities[idx])
                results.append(result)
        
        return results
    
    def save_index(self, file_path: str):
        """ä¿å­˜å‘é‡ç´¢å¼•"""
        if self.embeddings is not None:
            with open(file_path, 'wb') as f:
                pickle.dump({
                    'embeddings': self.embeddings,
                    'documents': self.documents
                }, f)
    
    def load_index(self, file_path: str):
        """åŠ è½½å‘é‡ç´¢å¼•"""
        if os.path.exists(file_path):
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
                self.embeddings = data['embeddings']
                self.documents = data['documents']
            return True
        return False

class KeywordSearchEngine:
    """å…³é”®è¯æœç´¢å¼•æ“ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    
    def __init__(self):
        self.documents = []
    
    def build_index(self, documents: List[Dict]):
        """æ„å»ºå…³é”®è¯ç´¢å¼•"""
        self.documents = documents
        logger.info(f"å…³é”®è¯æœç´¢å¼•æ“å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """å…³é”®è¯æœç´¢"""
        query_words = set(query.lower().split())
        results = []
        
        for doc in self.documents:
            content_words = set(doc['content'].lower().split())
            # è®¡ç®—å…³é”®è¯é‡å åº¦
            overlap = len(query_words.intersection(content_words))
            if overlap > 0:
                score = overlap / len(query_words)
                result = doc.copy()
                result['score'] = score
                results.append(result)
        
        # æŒ‰å¾—åˆ†æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]

class LocalKnowledgeBase:
    """æœ¬åœ°çŸ¥è¯†åº“ä¸»ç±»"""
    
    def __init__(self, knowledge_dir: str = "knowledge_base"):
        self.knowledge_dir = knowledge_dir
        self.documents = []
        self.metadata = {}
        
        # åˆå§‹åŒ–æœç´¢å¼•æ“
        if VECTOR_AVAILABLE:
            try:
                self.search_engine = VectorSearchEngine()
                self.search_type = "vector"
                logger.info("ä½¿ç”¨å‘é‡æœç´¢å¼•æ“")
            except Exception as e:
                logger.warning(f"å‘é‡æœç´¢åˆå§‹åŒ–å¤±è´¥: {e}")
                self.search_engine = KeywordSearchEngine()
                self.search_type = "keyword"
        else:
            self.search_engine = KeywordSearchEngine()
            self.search_type = "keyword"
            logger.info("ä½¿ç”¨å…³é”®è¯æœç´¢å¼•æ“")
        
        self.ensure_directories()
        self.load_knowledge_base()
    
    def ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        dirs = [
            self.knowledge_dir,
            f"{self.knowledge_dir}/documents",
            f"{self.knowledge_dir}/metadata",
            f"{self.knowledge_dir}/indices"
        ]
        
        for dir_path in dirs:
            os.makedirs(dir_path, exist_ok=True)
    
    def get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hasher = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except:
            return ""
    
    def add_document(self, file_path: str, title: str = None, metadata: Dict = None) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        if not os.path.exists(file_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»æ·»åŠ è¿‡
        file_hash = self.get_file_hash(file_path)
        if file_hash in self.metadata:
            logger.info(f"æ–‡ä»¶å·²å­˜åœ¨äºçŸ¥è¯†åº“: {file_path}")
            return True
        
        # æå–æ–‡æœ¬å†…å®¹
        content, file_type = DocumentProcessor.extract_text(file_path)
        if not content:
            logger.error(f"æ— æ³•æå–æ–‡ä»¶å†…å®¹: {file_path}")
            return False
        
        # åˆ†å‰²æ–‡æœ¬
        chunks = TextSplitter.split_by_paragraphs(content, max_length=500, overlap=50)
        
        title = title or os.path.basename(file_path)
        doc_id = len(self.documents)
        
        # æ·»åŠ æ–‡æ¡£ç‰‡æ®µ
        for i, chunk in enumerate(chunks):
            doc = {
                'id': f"{doc_id}_{i}",
                'title': f"{title} (ç¬¬{i+1}æ®µ)",
                'content': chunk,
                'source': file_path,
                'file_type': file_type,
                'chunk_index': i,
                'total_chunks': len(chunks),
                'created_at': datetime.now().isoformat(),
                'file_hash': file_hash
            }
            
            if metadata:
                doc.update(metadata)
            
            self.documents.append(doc)
        
        # ä¿å­˜å…ƒæ•°æ®
        self.metadata[file_hash] = {
            'file_path': file_path,
            'title': title,
            'file_type': file_type,
            'chunks_count': len(chunks),
            'added_at': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… å·²æ·»åŠ æ–‡æ¡£: {title} ({len(chunks)}ä¸ªç‰‡æ®µ)")
        return True
    
    def add_directory(self, dir_path: str, recursive: bool = True) -> int:
        """æ‰¹é‡æ·»åŠ ç›®å½•ä¸­çš„æ–‡æ¡£"""
        if not os.path.exists(dir_path):
            logger.error(f"ç›®å½•ä¸å­˜åœ¨: {dir_path}")
            return 0
        
        added_count = 0
        supported_extensions = {'.pdf', '.docx', '.doc', '.txt', '.md', '.csv', '.py', '.js', '.html'}
        
        for root, dirs, files in os.walk(dir_path):
            for file in files:
                file_path = os.path.join(root, file)
                ext = os.path.splitext(file)[1].lower()
                
                if ext in supported_extensions:
                    if self.add_document(file_path):
                        added_count += 1
            
            if not recursive:
                break
        
        logger.info(f"âœ… æ‰¹é‡æ·»åŠ å®Œæˆï¼Œå…±æ·»åŠ  {added_count} ä¸ªæ–‡æ¡£")
        return added_count
    
    def build_index(self):
        """æ„å»ºæœç´¢ç´¢å¼•"""
        if not self.documents:
            logger.warning("æ²¡æœ‰æ–‡æ¡£ï¼Œæ— æ³•æ„å»ºç´¢å¼•")
            return
        
        self.search_engine.build_index(self.documents)
        self.save_index()
        logger.info("æœç´¢ç´¢å¼•æ„å»ºå®Œæˆ")
    
    def search(self, query: str, top_k: int = 5, file_type: str = None) -> List[Dict]:
        """æœç´¢çŸ¥è¯†åº“"""
        if not self.documents:
            return []
        
        # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶ç±»å‹ï¼Œå…ˆè¿‡æ»¤
        documents = self.documents
        if file_type:
            documents = [doc for doc in self.documents if doc.get('file_type') == file_type]
            
            # ä¸´æ—¶æ„å»ºç´¢å¼•
            if documents:
                temp_engine = VectorSearchEngine() if self.search_type == "vector" else KeywordSearchEngine()
                temp_engine.build_index(documents)
                return temp_engine.search(query, top_k)
        
        return self.search_engine.search(query, top_k)
    
    def get_context_for_query(self, query: str, max_context_length: int = 1500) -> str:
        """ä¸ºæŸ¥è¯¢è·å–ç›¸å…³ä¸Šä¸‹æ–‡"""
        results = self.search(query, top_k=3)
        
        if not results:
            return ""
        
        context = "ğŸ“š åŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„ç›¸å…³ä¿¡æ¯ï¼š\n\n"
        current_length = len(context)
        
        for i, result in enumerate(results, 1):
            section = f"{i}. ã€{result['title']}ã€‘\n{result['content']}\n\n"
            
            if current_length + len(section) > max_context_length:
                break
            
            context += section
            current_length += len(section)
        
        context += "---\nè¯·åŸºäºä»¥ä¸ŠçŸ¥è¯†åº“ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ­£å¸¸å›ç­”ã€‚\n\n"
        return context
    
    def save_knowledge_base(self):
        """ä¿å­˜çŸ¥è¯†åº“"""
        # ä¿å­˜æ–‡æ¡£
        docs_path = f"{self.knowledge_dir}/documents/documents.json"
        with open(docs_path, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å…ƒæ•°æ®
        meta_path = f"{self.knowledge_dir}/metadata/metadata.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        
        logger.info("çŸ¥è¯†åº“å·²ä¿å­˜")
    
    def load_knowledge_base(self):
        """åŠ è½½çŸ¥è¯†åº“"""
        docs_path = f"{self.knowledge_dir}/documents/documents.json"
        meta_path = f"{self.knowledge_dir}/metadata/metadata.json"
        
        if os.path.exists(docs_path):
            with open(docs_path, 'r', encoding='utf-8') as f:
                self.documents = json.load(f)
            logger.info(f"ğŸ“š å·²åŠ è½½ {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        
        if os.path.exists(meta_path):
            with open(meta_path, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
            logger.info(f"ğŸ“‹ å·²åŠ è½½ {len(self.metadata)} ä¸ªæ–‡ä»¶çš„å…ƒæ•°æ®")
        
        # åŠ è½½ç´¢å¼•
        self.load_index()
    
    def save_index(self):
        """ä¿å­˜æœç´¢ç´¢å¼•"""
        if self.search_type == "vector":
            index_path = f"{self.knowledge_dir}/indices/vector_index.pkl"
            self.search_engine.save_index(index_path)
    
    def load_index(self):
        """åŠ è½½æœç´¢ç´¢å¼•"""
        if self.search_type == "vector":
            index_path = f"{self.knowledge_dir}/indices/vector_index.pkl"
            if self.search_engine.load_index(index_path):
                logger.info("å‘é‡ç´¢å¼•å·²åŠ è½½")
                return
        
        # å¦‚æœæœ‰æ–‡æ¡£ä½†æ²¡æœ‰ç´¢å¼•ï¼Œé‡æ–°æ„å»º
        if self.documents:
            self.search_engine.build_index(self.documents)
    
    def get_statistics(self) -> Dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        file_types = {}
        total_chunks = len(self.documents)
        total_files = len(self.metadata)
        
        for doc in self.documents:
            file_type = doc.get('file_type', 'unknown')
            file_types[file_type] = file_types.get(file_type, 0) + 1
        
        return {
            'total_files': total_files,
            'total_chunks': total_chunks,
            'file_types': file_types,
            'search_engine': self.search_type,
            'last_updated': datetime.now().isoformat()
        }
    
    def list_documents(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£"""
        return list(self.metadata.values())
    
    def remove_document(self, file_path: str) -> bool:
        """ç§»é™¤æ–‡æ¡£"""
        file_hash = self.get_file_hash(file_path)
        if file_hash not in self.metadata:
            logger.warning(f"æ–‡æ¡£ä¸åœ¨çŸ¥è¯†åº“ä¸­: {file_path}")
            return False
        
        # ç§»é™¤æ–‡æ¡£ç‰‡æ®µ
        self.documents = [doc for doc in self.documents if doc['file_hash'] != file_hash]
        
        # ç§»é™¤å…ƒæ•°æ®
        del self.metadata[file_hash]
        
        # é‡æ–°æ„å»ºç´¢å¼•
        if self.documents:
            self.search_engine.build_index(self.documents)
        
        logger.info(f"âœ… å·²ç§»é™¤æ–‡æ¡£: {file_path}")
        return True

# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    # åˆ›å»ºçŸ¥è¯†åº“å®ä¾‹
    kb = LocalKnowledgeBase()
    
    # ç¤ºä¾‹ç”¨æ³•
    print("ğŸ§  æœ¬åœ°çŸ¥è¯†åº“ç³»ç»Ÿæµ‹è¯•")
    print(f"æœç´¢å¼•æ“: {kb.search_type}")
    print(f"å½“å‰æ–‡æ¡£æ•°é‡: {len(kb.documents)}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = kb.get_statistics()
    print("ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡:", json.dumps(stats, indent=2, ensure_ascii=False))
