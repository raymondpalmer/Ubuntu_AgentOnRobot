"""
ç®€åŒ–ç‰ˆæœ¬åœ°çŸ¥è¯†åº“ - æ— å‘é‡ä¾èµ–
çº¯å…³é”®è¯æœç´¢ï¼Œæ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
"""

import os
import json
import hashlib
from typing import List, Dict, Optional
from datetime import datetime
import logging

# æ–‡æ¡£å¤„ç†ç›¸å…³
try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleDocumentProcessor:
    """ç®€å•æ–‡æ¡£å¤„ç†å™¨"""
    
    @staticmethod
    def extract_text_from_pdf(file_path: str) -> str:
        """ä»PDFæ–‡ä»¶æå–æ–‡æœ¬"""
        if not PDF_AVAILABLE:
            return ""
        try:
            text = ""
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
            return text.strip()
        except Exception as e:
            logger.error(f"PDFå¤„ç†å¤±è´¥ {file_path}: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_docx(file_path: str) -> str:
        """ä»Wordæ–‡æ¡£æå–æ–‡æœ¬"""
        if not DOCX_AVAILABLE:
            return ""
        try:
            doc = Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text.strip()
        except Exception as e:
            logger.error(f"Wordæ–‡æ¡£å¤„ç†å¤±è´¥ {file_path}: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_txt(file_path: str) -> str:
        """ä»æ–‡æœ¬æ–‡ä»¶æå–æ–‡æœ¬"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read().strip()
        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–ç¼–ç 
            encodings = ['gbk', 'gb2312', 'latin-1']
            for enc in encodings:
                try:
                    with open(file_path, 'r', encoding=enc) as file:
                        return file.read().strip()
                except:
                    continue
            logger.error(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: ç¼–ç é—®é¢˜")
            return ""
        except Exception as e:
            logger.error(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {e}")
            return ""
    
    @classmethod
    def extract_text(cls, file_path: str) -> tuple[str, str]:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åæå–æ–‡æœ¬"""
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext == '.pdf':
            return cls.extract_text_from_pdf(file_path), 'PDF'
        elif ext in ['.docx', '.doc']:
            return cls.extract_text_from_docx(file_path), 'Word'
        elif ext in ['.txt', '.md', '.py', '.js', '.html', '.css', '.json']:
            return cls.extract_text_from_txt(file_path), 'æ–‡æœ¬'
        else:
            logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")
            return "", 'æœªçŸ¥'

class SimpleTextSplitter:
    """ç®€å•æ–‡æœ¬åˆ†å‰²å™¨"""
    
    @staticmethod
    def split_text(text: str, max_length: int = 500) -> List[str]:
        """æŒ‰æ®µè½å’Œé•¿åº¦åˆ†å‰²æ–‡æœ¬"""
        if len(text) <= max_length:
            return [text]
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            # å¦‚æœå•ä¸ªæ®µè½å¤ªé•¿ï¼ŒæŒ‰å¥å­åˆ†å‰²
            if len(paragraph) > max_length:
                sentences = [s.strip() for s in paragraph.split('ã€‚') if s.strip()]
                for sentence in sentences:
                    if len(current_chunk) + len(sentence) <= max_length:
                        current_chunk += sentence + "ã€‚"
                    else:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence + "ã€‚"
            else:
                if len(current_chunk) + len(paragraph) <= max_length:
                    current_chunk += paragraph + "\n\n"
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = paragraph + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

class SimpleKnowledgeBase:
    """ç®€åŒ–ç‰ˆæœ¬åœ°çŸ¥è¯†åº“"""
    
    def __init__(self, knowledge_dir: str = "simple_knowledge_base"):
        self.knowledge_dir = knowledge_dir
        self.documents = []
        self.metadata = {}
        
        self.ensure_directories()
        self.load_knowledge_base()
    
    def ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        dirs = [
            self.knowledge_dir,
            f"{self.knowledge_dir}/documents",
            f"{self.knowledge_dir}/metadata"
        ]
        
        for dir_path in dirs:
            os.makedirs(dir_path, exist_ok=True)
    
    def get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hasher = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except:
            return ""
    
    def add_document(self, file_path: str, title: str = None, category: str = None) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        if not os.path.exists(file_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»æ·»åŠ è¿‡
        file_hash = self.get_file_hash(file_path)
        if file_hash in self.metadata:
            logger.info(f"æ–‡ä»¶å·²å­˜åœ¨äºçŸ¥è¯†åº“: {file_path}")
            return True
        
        # æå–æ–‡æœ¬å†…å®¹
        content, file_type = SimpleDocumentProcessor.extract_text(file_path)
        if not content:
            logger.error(f"æ— æ³•æå–æ–‡ä»¶å†…å®¹: {file_path}")
            return False
        
        # åˆ†å‰²æ–‡æœ¬
        chunks = SimpleTextSplitter.split_text(content, max_length=500)
        
        title = title or os.path.basename(file_path)
        doc_id = len(self.documents)
        
        # æ·»åŠ æ–‡æ¡£ç‰‡æ®µ
        for i, chunk in enumerate(chunks):
            doc = {
                'id': f"{doc_id}_{i}",
                'title': f"{title} (ç¬¬{i+1}æ®µ)",
                'content': chunk,
                'source': file_path,
                'file_type': file_type,
                'category': category or 'æœªåˆ†ç±»',
                'chunk_index': i,
                'total_chunks': len(chunks),
                'created_at': datetime.now().isoformat(),
                'file_hash': file_hash
            }
            
            self.documents.append(doc)
        
        # ä¿å­˜å…ƒæ•°æ®
        self.metadata[file_hash] = {
            'file_path': file_path,
            'title': title,
            'file_type': file_type,
            'category': category or 'æœªåˆ†ç±»',
            'chunks_count': len(chunks),
            'added_at': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… å·²æ·»åŠ æ–‡æ¡£: {title} ({len(chunks)}ä¸ªç‰‡æ®µ)")
        return True
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """ç®€å•å…³é”®è¯æœç´¢"""
        if not self.documents:
            return []
        
        query_words = set(query.lower().split())
        results = []
        
        for doc in self.documents:
            content_lower = doc['content'].lower()
            title_lower = doc['title'].lower()
            
            # è®¡ç®—åŒ¹é…åº¦
            content_matches = sum(1 for word in query_words if word in content_lower)
            title_matches = sum(1 for word in query_words if word in title_lower)
            
            # æ ‡é¢˜åŒ¹é…æƒé‡æ›´é«˜
            total_score = content_matches + (title_matches * 2)
            
            if total_score > 0:
                result = doc.copy()
                result['score'] = total_score / len(query_words)
                results.append(result)
        
        # æŒ‰å¾—åˆ†æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]
    
    def get_context_for_query(self, query: str, max_context_length: int = 1000) -> str:
        """ä¸ºæŸ¥è¯¢è·å–ç›¸å…³ä¸Šä¸‹æ–‡"""
        results = self.search(query, top_k=2)
        
        if not results:
            return ""
        
        context = "ğŸ“š åŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„ç›¸å…³ä¿¡æ¯ï¼š\n\n"
        current_length = len(context)
        
        for i, result in enumerate(results, 1):
            section = f"{i}. ã€{result['title']}ã€‘\n{result['content']}\n\n"
            
            if current_length + len(section) > max_context_length:
                break
            
            context += section
            current_length += len(section)
        
        context += "---\nè¯·åŸºäºä»¥ä¸ŠçŸ¥è¯†åº“ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ­£å¸¸å›ç­”ã€‚\n\n"
        return context
    
    def save_knowledge_base(self):
        """ä¿å­˜çŸ¥è¯†åº“"""
        # ä¿å­˜æ–‡æ¡£
        docs_path = f"{self.knowledge_dir}/documents/documents.json"
        with open(docs_path, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å…ƒæ•°æ®
        meta_path = f"{self.knowledge_dir}/metadata/metadata.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        
        logger.info("çŸ¥è¯†åº“å·²ä¿å­˜")
    
    def load_knowledge_base(self):
        """åŠ è½½çŸ¥è¯†åº“"""
        docs_path = f"{self.knowledge_dir}/documents/documents.json"
        meta_path = f"{self.knowledge_dir}/metadata/metadata.json"
        
        try:
            if os.path.exists(docs_path):
                with open(docs_path, 'r', encoding='utf-8') as f:
                    self.documents = json.load(f)
                logger.info(f"ğŸ“š å·²åŠ è½½ {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            
            if os.path.exists(meta_path):
                with open(meta_path, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                logger.info(f"ğŸ“‹ å·²åŠ è½½ {len(self.metadata)} ä¸ªæ–‡ä»¶çš„å…ƒæ•°æ®")
        except Exception as e:
            logger.error(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
    
    def list_documents(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£"""
        return list(self.metadata.values())
    
    def get_statistics(self) -> Dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        file_types = {}
        categories = {}
        
        for doc in self.documents:
            file_type = doc.get('file_type', 'æœªçŸ¥')
            category = doc.get('category', 'æœªåˆ†ç±»')
            
            file_types[file_type] = file_types.get(file_type, 0) + 1
            categories[category] = categories.get(category, 0) + 1
        
        return {
            'total_files': len(self.metadata),
            'total_chunks': len(self.documents),
            'file_types': file_types,
            'categories': categories,
            'last_updated': datetime.now().isoformat()
        }

# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    # åˆ›å»ºçŸ¥è¯†åº“å®ä¾‹
    kb = SimpleKnowledgeBase()
    
    # ç¤ºä¾‹ç”¨æ³•
    print("ğŸ§  ç®€åŒ–ç‰ˆæœ¬åœ°çŸ¥è¯†åº“ç³»ç»Ÿæµ‹è¯•")
    print(f"å½“å‰æ–‡æ¡£æ•°é‡: {len(kb.documents)}")
    
    # æµ‹è¯•æœç´¢
    if kb.documents:
        test_query = "æœºå™¨äºº"
        print(f"\nğŸ” æµ‹è¯•æœç´¢: '{test_query}'")
        results = kb.search(test_query)
        for i, result in enumerate(results, 1):
            print(f"{i}. {result['title']} (å¾—åˆ†: {result['score']:.2f})")
            print(f"   {result['content'][:100]}...")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = kb.get_statistics()
    print(f"\nğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡: {json.dumps(stats, indent=2, ensure_ascii=False)}")
